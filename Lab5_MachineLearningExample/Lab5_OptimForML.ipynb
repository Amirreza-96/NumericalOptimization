{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"Fig/UGA.png\" width=\"30%\" height=\"30%\"></center>\n",
    "<center><h3>Master of Science in Industrial and Applied Mathematics (MSIAM)  - 1st year</h3></center>\n",
    "<hr>\n",
    "<center><h1>Numerical Optimization</h1></center>\n",
    "<center><h2>Lab 5: Optimization for ML </h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Algorithms performance on practical problems\n",
    "\n",
    "In this lab, we will investigate how to evaluate and display performance of optimization algorithms over a practical problem of machine learning: binary classification using logistic regression.</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning as an Optimization problem\n",
    "\n",
    "We have some *data*  $\\mathcal{D}$ consisting of $m$ *examples* $\\{d_i\\}$; each example consisting of a *feature* vector $a_i\\in\\mathbb{R}^d$ and an *observation* $b_i\\in \\mathcal{O}$: $\\mathcal{D} = \\{[a_i,b_i]\\}_{i=1..m}$. In this lab, we will consider the <a href=\"https://archive.ics.uci.edu/ml/machine-learning-databases/ionosphere/ionosphere.names\">ionosphere</a> dataset.\n",
    " \n",
    "The goal of *supervised learning* is to construct a predictor for the observations when given feature vectors.\n",
    "\n",
    "A popular approach is based on *linear models* which are based on finding a *parameter* $x$ such that the real number $\\langle a_i , x \\rangle$ is used to predict the value of the observation through a *predictor function* $g:\\mathbb{R}\\to \\mathcal{O}$: $g(\\langle a_i , x \\rangle)$ is the predicted value from $a_i$.\n",
    "\n",
    "In order to find such a parameter, we use the available data and a *loss* $\\ell$ that penalizes the error made between the predicted $g(\\langle a_i , x \\rangle)$ and observed $b_i$ values. For each example $i$, the corresponding error function for a parameter $x$ is $f_i(x) =   \\ell( g(\\langle a_i , x \\rangle) ; b_i )$. Using the whole data, the parameter that minimizes the total error is the solution of the minimization problem\n",
    "\n",
    "$$ \\min_{x\\in\\mathbb{R}^d}  \\frac{1}{m} \\sum_{i=1}^m f_i(x) = \\frac{1}{m} \\sum_{i=1}^m  \\ell( g(\\langle a_i , x \\rangle) ; b_i ). $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification with Logisitic Regression\n",
    "\n",
    "In our setup, the observations are binary: $\\mathcal{O} = \\{-1 , +1 \\}$, and the *Logistic loss* is used to form the following optimization problem\n",
    "\\begin{align*}\n",
    "\\min_{x\\in\\mathbb{R}^d } f(x) := \\frac{1}{m}  \\sum_{i=1}^m  \\log( 1+\\exp(-b_i \\langle a_i,x \\rangle) ) + \\frac{\\lambda_2}{2} \\|x\\|_2^2.\n",
    "\\end{align*}\n",
    "where the last term is added as a regularization (of type $\\ell_2$, aka Tikhnov) to prevent overfitting.\n",
    "\n",
    "Under some statistical hypotheses, $x^\\star = \\arg\\min f(x)$ maximizes the likelihood of the labels knowing the features vector. Then, for a new point $d$ with features vector $a$, \n",
    "$$ p_1(a) = \\mathbb{P}[d\\in \\text{ class }  +1] = \\frac{1}{1+\\exp(-\\langle a;x^\\star \\rangle)} $$\n",
    "\n",
    "Thus, from $a$, if $p_1(a)$ is close to $1$, one can decide that $d$ belongs to class $1$; and the opposite decision if $p(a)$ is close to $0$. Between the two, the appreciation is left to the data scientist depending on the application.\n",
    "\n",
    "## Objective of the optimizer\n",
    " \n",
    "Given oracles for the function and its gradient, as well as an upper-bound of the Lipschitz constant $L$ of the gradient, find a minimizer of $f$.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You are given *all* oracles of $f$ (function, gradient, Hessian) in `logistic_regression_ionosphere.py` and several algorithms in `algoGradient.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Influence of strong convexity on the speed of the gradient method\n",
    "\n",
    "\n",
    "> Run the following  blocks for different values of parameter `lam2` of the problem. What do you notice in terms of speed of convergence, what is the reason?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from algoGradient import *  # import all methods of the module into the current environment\n",
    "import numpy as np\n",
    "import logistic_regression_ionosphere as pb\n",
    "\n",
    "\n",
    "#### Parameter we give at our algorithm (see algoGradient.ipynb)\n",
    "PREC    = 1e-5                     # Sought precision\n",
    "ITE_MAX = 5000                      # Max number of iterations\n",
    "x0      = np.zeros(pb.n)              # Initial point\n",
    "step    = 1.0/pb.L\n",
    "\n",
    "pb.lam2 = 0.1\n",
    "\n",
    "##### gradient algorithm\n",
    "x,x_tab = gradient_algorithm(pb.f , pb.f_grad , x0 , step , PREC , ITE_MAX )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "F = []\n",
    "for i in range(x_tab.shape[0]):\n",
    "    F.append( pb.f(x_tab[i])) \n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot( F, color=\"black\", linewidth=1.0, linestyle=\"-\",label='gradient')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accelerating poorly conditioned problems\n",
    "\n",
    "While the addition of strong convexity accelerates the rate in practice, it usually result shift the solutions of the original problem. For a learning problem, it affects the accuracy.\n",
    "\n",
    "In order to get faster convergences when the rate is slower, several acceleration techniques exist. We are going to present the most common in the following.\n",
    "\n",
    "### Nesterov's fast gradient\n",
    "\n",
    "In a series of papers published in the 80's, Yu. Nesterov proposed an acceleration technique in order to make the worst case rate of the gradient algorithm from $\\mathcal{O}(1/k)$ to  $\\mathcal{O}(1/k^2)$. This technique is now immensely popular, notably in the machine learning and image processing communities.\n",
    " \n",
    "\n",
    "The iterations of Nesterov's accelerated gradient are as such:\n",
    "$$ \\left\\{  \\begin{array}{ll}  x_{k+1} = y_k - \\gamma \\nabla f(y_k) \\\\ y_{k+1} = x_{k+1} + \\alpha_{k+1} (x_{k+1} - x_k )  \\end{array}           \\right. $$\n",
    "with \n",
    "$$ \\alpha_{k+1} = \\frac{\\lambda_k -1 }{\\lambda_{k+1}} \\text{ with } \\lambda_0 = 0 \\text{ and } \\lambda_{k+1} = \\frac{1+\\sqrt{1+4\\lambda_k^2}}{2} . $$\n",
    " \n",
    "Although no clear intuition can be drawn, the extended point can be seen as an extension by inertia of the last points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Implement Nesterov's fast gradient algorithm in `algoGradient.py`.\n",
    "\n",
    "> Run the constant stepsize and fast gradient algorithms and compare the convergence rates (for lam2 = 0.001)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from algoGradient import *  # import all methods of the module into the current environment\n",
    "\n",
    "import numpy as np\n",
    "import logistic_regression_ionosphere as pb\n",
    "\n",
    "#### Parameter we give at our algorithm (see algoGradient.ipynb)\n",
    "PREC    = 1e-5                     # Sought precision\n",
    "ITE_MAX = 5000                      # Max number of iterations\n",
    "x0      = np.zeros(pb.n)              # Initial point\n",
    "step    = 1.0/pb.L\n",
    "\n",
    "pb.lam2 = 0.001\n",
    "\n",
    "##### gradient algorithm\n",
    "x,x_tab = gradient_algorithm(pb.f , pb.f_grad , x0 , step , PREC , ITE_MAX )\n",
    "\n",
    "##### fast gradient algorithm\n",
    "xF,xF_tab  = fast_gradient_algorithm(pb.f , pb.f_grad , x0 , step , PREC , ITE_MAX )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "F = []\n",
    "G = []\n",
    "for i in range(x_tab.shape[0]):\n",
    "    F.append( pb.f(x_tab[i])) \n",
    "    G.append( np.linalg.norm(pb.f_grad(x_tab[i] )) )\n",
    "\n",
    "FF = []\n",
    "GF = []\n",
    "for i in range(xF_tab.shape[0]):\n",
    "    FF.append( pb.f(xF_tab[i])) \n",
    "    GF.append( np.linalg.norm(pb.f_grad(xF_tab[i] )) )\n",
    "\n",
    "plt.figure()\n",
    "plt.plot( F, color=\"black\", linewidth=1.0, linestyle=\"-\",label='gradient')\n",
    "plt.plot( FF, color=\"red\", linewidth=1.0, linestyle=\"-\",label='fast gradient')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot( G, color=\"black\", linewidth=1.0, linestyle=\"-\",label='gradient')\n",
    "plt.plot( GF, color=\"red\", linewidth=1.0, linestyle=\"-\",label='fast gradient')\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "### Other methods: line-search, BFGS\n",
    "\n",
    "\n",
    "Other popular methods to accelerate convergence are:\n",
    "* line-search (as seen quickly in the previous lab, it is implemented in 1.c of file `algoGradient.py` )\n",
    "* BFGS which is a Quasi-Newton method in the sense that it approximates second order information in an online setting. \n",
    "\n",
    "**BFGS.** (Broyden-Fletcher-Goldfarb-Shanno, 1970) The popular BFGS algorithm consist in performing the following iteration\n",
    "$$ x_{k+1}=x_k - \\gamma_k W_k \\nabla f(x_k)$$\n",
    "where $\\gamma_k$ is given by Wolfe's line-search and positive definite matrix $W_k$ is computed as\n",
    "$$ W_{k+1}=W_k - \\frac{s_k y_k^T W_k+W_k y_k s_k^T}{y_k^T s_k} +\\left[1+\\frac{y_k^T W_k y_k}{y_k^T s_k}\\right]\\frac{s_k s_k^T}{y_k^T s_k} $$\n",
    "with $s_k=x_{k+1}-x_{k}$ and $y_k=\\nabla f(x_{k+1}) - \\nabla f(x_{k})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Implement BFGS in Section 3 of  file `algoGradient.py` .\n",
    "\n",
    "> Compare the performance of the previously investigated algorithms. *(Note that you can also test the performance of Newton's method although it is a bit unfair compared to the other algorithms as the variable size is small)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from algoGradient import *  # import all methods of the module into the current environment\n",
    "\n",
    "import numpy as np\n",
    "import logistic_regression_ionosphere as pb\n",
    "\n",
    "#### Parameter we give at our algorithm (see algoGradient.ipynb)\n",
    "PREC    = 1e-5                     # Sought precision\n",
    "ITE_MAX = 500                      # Max number of iterations\n",
    "x0      = np.zeros(pb.n)              # Initial point\n",
    "step    = 1.0/pb.L\n",
    "\n",
    "##### gradient algorithm\n",
    "x,x_tab = gradient_algorithm(pb.f , pb.f_grad , x0 , step , PREC , ITE_MAX )\n",
    "\n",
    "##### fast gradient algorithm\n",
    "xF,xF_tab  = fast_gradient_algorithm(pb.f , pb.f_grad , x0 , step , PREC , ITE_MAX )\n",
    "\n",
    "##### Wolfe line-search algorithm\n",
    "xW,xW_tab = gradient_Wolfe(pb.f , pb.f_grad , x0 , PREC , ITE_MAX )\n",
    "\n",
    "##### BFGS algorithm\n",
    "xB,xB_tab = bfgs(pb.f , pb.f_grad , x0 , PREC , ITE_MAX )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "F = []\n",
    "G = []\n",
    "for i in range(x_tab.shape[0]):\n",
    "    F.append( pb.f(x_tab[i])) \n",
    "    G.append( np.linalg.norm(pb.f_grad(x_tab[i] )) )\n",
    "\n",
    "FF = []\n",
    "GF = []\n",
    "for i in range(xF_tab.shape[0]):\n",
    "    FF.append( pb.f(xF_tab[i])) \n",
    "    GF.append( np.linalg.norm(pb.f_grad(xF_tab[i] )) )\n",
    "    \n",
    "FW = []\n",
    "GW = []\n",
    "for i in range(xW_tab.shape[0]):\n",
    "    FW.append( pb.f(xW_tab[i])) \n",
    "    GW.append( np.linalg.norm(pb.f_grad(xW_tab[i] )) )\n",
    "    \n",
    "    \n",
    "FB = []\n",
    "GB = []\n",
    "for i in range(xB_tab.shape[0]):\n",
    "    FB.append( pb.f(xB_tab[i])) \n",
    "    GB.append( np.linalg.norm(pb.f_grad(xB_tab[i] )) )\n",
    "\n",
    "plt.figure()\n",
    "plt.plot( F, color=\"black\", linewidth=1.0, linestyle=\"-\",label='gradient')\n",
    "plt.plot( FF, color=\"red\", linewidth=1.0, linestyle=\"-\",label='fast gradient')\n",
    "plt.plot( FW, color=\"magenta\", linewidth=1.0, linestyle=\"-\",label='Wolfe')\n",
    "plt.plot( FB, color=\"green\", linewidth=1.0, linestyle=\"-\",label='BFGS')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot( G, color=\"black\", linewidth=1.0, linestyle=\"-\",label='gradient')\n",
    "plt.plot( GF, color=\"red\", linewidth=1.0, linestyle=\"-\",label='fast gradient')\n",
    "plt.plot( GW, color=\"magenta\", linewidth=1.0, linestyle=\"-\",label='Wolfe')\n",
    "plt.plot( GB, color=\"green\", linewidth=1.0, linestyle=\"-\",label='BFGS')\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Performance on learning problems\n",
    "\n",
    "### Prediction power\n",
    "\n",
    "\n",
    "\n",
    "Our problem of interest is binary classification using logistic regression.</br>\n",
    "Although this is a machine learning task, the predictor construction amounts to minimizing a smooth convex optimization function $f$ called the *loss*, the final minimizer is called a *predictor* and its scalar product with the data vector gives a probability of belonging to class $1$.\n",
    "\n",
    "The previous test was based on the functional decrease whereas our task is binary classification. Let us look at the final accuracies obtained.\n",
    "\n",
    "> The file `logistic_regression.py` contains a `prediction` function that takes a *predictor* and resturn the accuracy of the predictor. Take a look at how the function is defined.\n",
    "\n",
    "> Observe the accuracy of all final points obtained before. What do you notice? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred,perf = pb.prediction(x,PRINT=False)\n",
    "print(\"Gradient algorithm: \\t{:.2f}%\".format(perf*100))\n",
    "\n",
    "predF,perfF = pb.prediction(xF,PRINT=False)\n",
    "print(\"Fast Gradient: \\t\\t{:.2f}%\".format(perfF*100))\n",
    "\n",
    "predW,perfW = pb.prediction(xW,PRINT=False)\n",
    "print(\"Wolfe: \\t\\t\\t{:.2f}%\".format(perfW*100))\n",
    "\n",
    "predB,perfB = pb.prediction(xB,PRINT=False)\n",
    "print(\"BFGS: \\t\\t\\t{:.2f}%\".format(perfB*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predF,perfF = pb.prediction(xF,PRINT=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
