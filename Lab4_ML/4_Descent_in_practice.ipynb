{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"Fig/UGA.png\" width=\"30%\" height=\"30%\"></center>\n",
    "<center><h3>Master of Science in Industrial and Applied Mathematics (MSIAM)  - 1st year</h3></center>\n",
    "<hr>\n",
    "<center><h1>Numerical Optimization</h1></center>\n",
    "<center><h2>Lab on practical descent methods </h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "The following script will allow you to import *notebooks* as if you imported *python files* and will have to be executed at each time you launch Jupyter notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import start\n",
    "from imp import reload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Algorithms performance on practical problems\n",
    "\n",
    "In this lab, we will investigate how to evaluate and display performance of optimization algorithms over a practical problem of machine learning: binary classification using logistic regression.</br>\n",
    "\n",
    "\n",
    "### Influence of strong convexity on the speed of the gradient method\n",
    "\n",
    "\n",
    "> The file `logistic_regression_ionosphere.ipynb` contains the simulators for the loss function. \n",
    "\n",
    "> Run the following two blocks for different values of parameter `lam` of the problem. What do you notice in terms of speed of convergence, what is the reason?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import algoGradient         # load our algoGradient module (from notebook)\n",
    "reload(algoGradient)        # reload the module if changed (and saved)\n",
    "from algoGradient import *  # import all methods of the module into the current environment\n",
    "\n",
    "import numpy as np\n",
    "import logistic_regression_ionosphere as pb\n",
    "reload(pb)\n",
    "\n",
    "#### Parameter we give at our algorithm (see algoGradient.ipynb)\n",
    "PREC    = 1e-5                     # Sought precision\n",
    "ITE_MAX = 5000                      # Max number of iterations\n",
    "x0      = np.zeros(pb.n)              # Initial point\n",
    "step    = 1.0/pb.L\n",
    "\n",
    "##### gradient algorithm\n",
    "x,x_tab = gradient_algorithm(pb.f , pb.f_grad , x0 , step , PREC , ITE_MAX )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "F = []\n",
    "for i in range(x_tab.shape[0]):\n",
    "    F.append( pb.f(x_tab[i])) \n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot( F, color=\"black\", linewidth=1.0, linestyle=\"-\",label='gradient')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accelerating poorly conditioned problems\n",
    "\n",
    "While the addition of strong convexity accelerates the rate in practice, it usually result shift the solutions of the original problem. For a learning problem, it affects the accuracy.\n",
    "\n",
    "In order to get faster convergences when the rate is slower, several acceleration techniques exist. We are going to present the most common in the following.\n",
    "\n",
    "### Nesterov's fast gradient\n",
    "\n",
    "> Take a look at the definition of *Nesterov's fast gradient algorithm* in Section 1.d of file `algoGradient.ipynb` and implement it.\n",
    "\n",
    "> Run the constant stepsize and fast gradient algorithms and compare the convergence rates (for lam = 0.001)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import algoGradient         # load our algoGradient module (from notebook)\n",
    "reload(algoGradient)        # reload the module if changed (and saved)\n",
    "from algoGradient import *  # import all methods of the module into the current environment\n",
    "\n",
    "import numpy as np\n",
    "import logistic_regression_ionosphere as pb\n",
    "reload(pb)\n",
    "\n",
    "#### Parameter we give at our algorithm (see algoGradient.ipynb)\n",
    "PREC    = 1e-5                     # Sought precision\n",
    "ITE_MAX = 5000                      # Max number of iterations\n",
    "x0      = np.zeros(pb.n)              # Initial point\n",
    "step    = 1.0/pb.L\n",
    "\n",
    "##### gradient algorithm\n",
    "x,x_tab = gradient_algorithm(pb.f , pb.f_grad , x0 , step , PREC , ITE_MAX )\n",
    "\n",
    "##### fast gradient algorithm\n",
    "xF,xF_tab  = fast_gradient_algorithm(pb.f , pb.f_grad , x0 , step , PREC , ITE_MAX )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "F = []\n",
    "G = []\n",
    "for i in range(x_tab.shape[0]):\n",
    "    F.append( pb.f(x_tab[i])) \n",
    "    G.append( np.linalg.norm(pb.f_grad(x_tab[i] )) )\n",
    "\n",
    "FF = []\n",
    "GF = []\n",
    "for i in range(xF_tab.shape[0]):\n",
    "    FF.append( pb.f(xF_tab[i])) \n",
    "    GF.append( np.linalg.norm(pb.f_grad(xF_tab[i] )) )\n",
    "\n",
    "plt.figure()\n",
    "plt.plot( F, color=\"black\", linewidth=1.0, linestyle=\"-\",label='gradient')\n",
    "plt.plot( FF, color=\"red\", linewidth=1.0, linestyle=\"-\",label='fast gradient')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot( G, color=\"black\", linewidth=1.0, linestyle=\"-\",label='gradient')\n",
    "plt.plot( GF, color=\"red\", linewidth=1.0, linestyle=\"-\",label='fast gradient')\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "### Other methods: line-search, BFGS\n",
    "\n",
    "\n",
    "Other popular methods to accelerate convergence are:\n",
    "* line-search (as seen quickly in the previous lab, it is implemented in 1.c of file `algoGradient.ipynb` )\n",
    "* BFGS which is a Quasi-Newton method in the sense that it approximates second order information in an online setting. \n",
    "\n",
    "> Implement BFGS in Section 3 of  file `algoGradient.ipynb` .\n",
    "\n",
    "> Compare the performance of the previously investigated algorithms. *(Note that you can also test the performance of Newton's method although it is a bit unfair compared to the other algorithms as the variable size is small)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import algoGradient         # load our algoGradient module (from notebook)\n",
    "reload(algoGradient)        # reload the module if changed (and saved)\n",
    "from algoGradient import *  # import all methods of the module into the current environment\n",
    "\n",
    "import numpy as np\n",
    "import logistic_regression_ionosphere as pb\n",
    "reload(pb)\n",
    "\n",
    "#### Parameter we give at our algorithm (see algoGradient.ipynb)\n",
    "PREC    = 1e-5                     # Sought precision\n",
    "ITE_MAX = 500                      # Max number of iterations\n",
    "x0      = np.zeros(pb.n)              # Initial point\n",
    "step    = 1.0/pb.L\n",
    "\n",
    "##### gradient algorithm\n",
    "x,x_tab = gradient_algorithm(pb.f , pb.f_grad , x0 , step , PREC , ITE_MAX )\n",
    "\n",
    "##### fast gradient algorithm\n",
    "xF,xF_tab  = fast_gradient_algorithm(pb.f , pb.f_grad , x0 , step , PREC , ITE_MAX )\n",
    "\n",
    "##### Wolfe line-search algorithm\n",
    "xW,xW_tab = gradient_Wolfe(pb.f , pb.f_grad , x0 , PREC , ITE_MAX )\n",
    "\n",
    "##### BFGS algorithm\n",
    "xB,xB_tab = bfgs(pb.f , pb.f_grad , x0 , PREC , ITE_MAX )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "F = []\n",
    "G = []\n",
    "for i in range(x_tab.shape[0]):\n",
    "    F.append( pb.f(x_tab[i])) \n",
    "    G.append( np.linalg.norm(pb.f_grad(x_tab[i] )) )\n",
    "\n",
    "FF = []\n",
    "GF = []\n",
    "for i in range(xF_tab.shape[0]):\n",
    "    FF.append( pb.f(xF_tab[i])) \n",
    "    GF.append( np.linalg.norm(pb.f_grad(xF_tab[i] )) )\n",
    "    \n",
    "FW = []\n",
    "GW = []\n",
    "for i in range(xW_tab.shape[0]):\n",
    "    FW.append( pb.f(xW_tab[i])) \n",
    "    GW.append( np.linalg.norm(pb.f_grad(xW_tab[i] )) )\n",
    "    \n",
    "    \n",
    "FB = []\n",
    "GB = []\n",
    "for i in range(xB_tab.shape[0]):\n",
    "    FB.append( pb.f(xB_tab[i])) \n",
    "    GB.append( np.linalg.norm(pb.f_grad(xB_tab[i] )) )\n",
    "\n",
    "plt.figure()\n",
    "plt.plot( F, color=\"black\", linewidth=1.0, linestyle=\"-\",label='gradient')\n",
    "plt.plot( FF, color=\"red\", linewidth=1.0, linestyle=\"-\",label='fast gradient')\n",
    "plt.plot( FW, color=\"magenta\", linewidth=1.0, linestyle=\"-\",label='Wolfe')\n",
    "plt.plot( FB, color=\"green\", linewidth=1.0, linestyle=\"-\",label='BFGS')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot( G, color=\"black\", linewidth=1.0, linestyle=\"-\",label='gradient')\n",
    "plt.plot( GF, color=\"red\", linewidth=1.0, linestyle=\"-\",label='fast gradient')\n",
    "plt.plot( GW, color=\"magenta\", linewidth=1.0, linestyle=\"-\",label='Wolfe')\n",
    "plt.plot( GB, color=\"green\", linewidth=1.0, linestyle=\"-\",label='BFGS')\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Performance on learning problems\n",
    "\n",
    "### Prediction power\n",
    "\n",
    "\n",
    "\n",
    "Our problem of interest is binary classification using logistic regression.</br>\n",
    "Although this is a machine learning task, the predictor construction amounts to minimizing a smooth convex optimization function $f$ called the *loss*, the final minimizer is called a *predictor* and its scalar product with the data vector gives a probability of belonging to class $1$.\n",
    "\n",
    "The previous test was based on the functional decrease whereas our task is binary classification. Let us look at the final accuracies obtained.\n",
    "\n",
    "> The file `logistic_regression.ipynb` also contains a `prediction` function that takes a *predictor* and resturn the accuracy of the predictor. Take a look at how the function is defined.\n",
    "\n",
    "> Observe the accuracy of all final points obtained before. What do you notice? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred,perf = pb.prediction(x,PRINT=False)\n",
    "print(\"Gradient algorithm: \\t{:.2f}%\".format(perf*100))\n",
    "\n",
    "predF,perfF = pb.prediction(xF,PRINT=False)\n",
    "print(\"Fast Gradient: \\t\\t{:.2f}%\".format(perfF*100))\n",
    "\n",
    "predW,perfW = pb.prediction(xW,PRINT=False)\n",
    "print(\"Wolfe: \\t\\t\\t{:.2f}%\".format(perfW*100))\n",
    "\n",
    "predB,perfB = pb.prediction(xB,PRINT=False)\n",
    "print(\"BFGS: \\t\\t\\t{:.2f}%\".format(perfB*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predF,perfF = pb.prediction(xF,PRINT=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
